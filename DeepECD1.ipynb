{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe2bb17-5408-4290-bdbd-44d568bc7d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "400d8030-31c6-4932-b452-8ca34769f72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def comparison(testlabel, resultslabel):\n",
    "\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for row1 in range(len(resultslabel)):\n",
    "        if resultslabel[row1] < 0.5:\n",
    "            resultslabel[row1] = 0\n",
    "        else:\n",
    "            resultslabel[row1] = 1\n",
    "\n",
    "    for row2 in range(len(testlabel)):\n",
    "\n",
    "        if testlabel[row2] == 1 and testlabel[row2] == resultslabel[row2]:\n",
    "            TP = TP + 1\n",
    "        if testlabel[row2] == 0 and testlabel[row2] != resultslabel[row2]:\n",
    "            FP = FP + 1\n",
    "        if testlabel[row2] == 0 and testlabel[row2] == resultslabel[row2]:\n",
    "            TN = TN + 1\n",
    "        if testlabel[row2] == 1 and testlabel[row2] != resultslabel[row2]:\n",
    "            FN = FN + 1\n",
    "\n",
    "    # ACC：accuracy\n",
    "    if TP + TN + FP + FN != 0:\n",
    "        ACC = (TP + TN) / (TP + TN + FP + FN)\n",
    "    else:\n",
    "        ACC = 999999\n",
    "\n",
    "    # F1 score：is the harmonic mean of precision and sensitivity\n",
    "    if TP + FP + FN != 0:\n",
    "        F1 = (2 * TP) / (2 * TP + FP + FN)\n",
    "    else:\n",
    "        F1 = 999999\n",
    "\n",
    "    # MCC：Matthews correlation coefficient\n",
    "    if (TP + FP) * (TP + FN) * (TN + FP) * (TN + FN) != 0:\n",
    "        MCC = (TP * TN - FP * FN) / math.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))\n",
    "    else:\n",
    "        MCC = 999999\n",
    "\n",
    "    return TP, FP, TN, FN, ACC, F1, MCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b0900cb-fdf8-4d59-a963-6da6b512c5ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35980, 200, 4)\n",
      "(3998, 200, 4)\n",
      "(None, 247, 1)\n",
      "Epoch 1/20\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.5389 - accuracy: 0.7541INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 19s 27ms/step - loss: 0.5389 - accuracy: 0.7541 - val_loss: 0.5091 - val_accuracy: 0.7685 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "504/506 [============================>.] - ETA: 0s - loss: 0.4497 - accuracy: 0.8195INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 13s 25ms/step - loss: 0.4493 - accuracy: 0.8197 - val_loss: 0.4280 - val_accuracy: 0.8307 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "505/506 [============================>.] - ETA: 0s - loss: 0.4234 - accuracy: 0.8326INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 12s 25ms/step - loss: 0.4233 - accuracy: 0.8328 - val_loss: 0.4136 - val_accuracy: 0.8366 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "506/506 [==============================] - 8s 16ms/step - loss: 0.4091 - accuracy: 0.8396 - val_loss: 0.4182 - val_accuracy: 0.8310 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "506/506 [==============================] - 8s 16ms/step - loss: 0.4000 - accuracy: 0.8442 - val_loss: 0.4295 - val_accuracy: 0.8257 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "506/506 [==============================] - 8s 16ms/step - loss: 0.3955 - accuracy: 0.8478 - val_loss: 0.4673 - val_accuracy: 0.8077 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "504/506 [============================>.] - ETA: 0s - loss: 0.3820 - accuracy: 0.8552INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 12s 25ms/step - loss: 0.3821 - accuracy: 0.8551 - val_loss: 0.3993 - val_accuracy: 0.8419 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "506/506 [==============================] - 8s 16ms/step - loss: 0.3711 - accuracy: 0.8621 - val_loss: 0.4503 - val_accuracy: 0.8255 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "506/506 [==============================] - 8s 16ms/step - loss: 0.3663 - accuracy: 0.8655 - val_loss: 0.4038 - val_accuracy: 0.8377 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "504/506 [============================>.] - ETA: 0s - loss: 0.3622 - accuracy: 0.8656INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 13s 25ms/step - loss: 0.3622 - accuracy: 0.8656 - val_loss: 0.4032 - val_accuracy: 0.8432 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "506/506 [==============================] - 8s 16ms/step - loss: 0.3581 - accuracy: 0.8676 - val_loss: 0.4061 - val_accuracy: 0.8374 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "504/506 [============================>.] - ETA: 0s - loss: 0.3498 - accuracy: 0.8734INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 13s 25ms/step - loss: 0.3496 - accuracy: 0.8735 - val_loss: 0.4057 - val_accuracy: 0.8438 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "504/506 [============================>.] - ETA: 0s - loss: 0.3451 - accuracy: 0.8760INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 13s 25ms/step - loss: 0.3451 - accuracy: 0.8760 - val_loss: 0.3917 - val_accuracy: 0.8524 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "506/506 [==============================] - 8s 16ms/step - loss: 0.3408 - accuracy: 0.8770 - val_loss: 0.4283 - val_accuracy: 0.8335 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "506/506 [==============================] - 8s 15ms/step - loss: 0.3370 - accuracy: 0.8796 - val_loss: 0.4265 - val_accuracy: 0.8319 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "506/506 [==============================] - 8s 16ms/step - loss: 0.3352 - accuracy: 0.8810 - val_loss: 0.4158 - val_accuracy: 0.8416 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "506/506 [==============================] - 8s 16ms/step - loss: 0.3263 - accuracy: 0.8828 - val_loss: 0.4052 - val_accuracy: 0.8374 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "506/506 [==============================] - 8s 16ms/step - loss: 0.3209 - accuracy: 0.8880 - val_loss: 0.4473 - val_accuracy: 0.8307 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "506/506 [==============================] - 8s 16ms/step - loss: 0.3245 - accuracy: 0.8853 - val_loss: 0.4079 - val_accuracy: 0.8466 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "506/506 [==============================] - 8s 16ms/step - loss: 0.3305 - accuracy: 0.8835 - val_loss: 0.4208 - val_accuracy: 0.8407 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 10:07:58.323492: W tensorflow/core/util/tensor_slice_reader.cc:97] Could not open ./model/bestmodel: FAILED_PRECONDITION: model/bestmodel; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 1s 5ms/step\n",
      "PLeccNET\n",
      "TP: 1710 FP: 289 TN: 1676 FN: 323\n",
      "ACC: 0.8469234617308654 F1: 0.8482142857142857 MCC: 0.6939473064586114\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, MaxPool1D, Flatten, Dropout, Conv1D,Reshape,Input,AveragePooling1D,LSTM,Attention,MultiHeadAttention,MaxPooling1D,GlobalMaxPooling1D,Concatenate,BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers,Model,optimizers\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.losses import binary_crossentropy, hinge\n",
    "callback4 = [callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10,\n",
    "                                             verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0),\n",
    "                 callbacks.ModelCheckpoint(filepath=r'./model/bestmodel', monitor='val_accuracy', verbose=0,mode = 'max',\n",
    "                                           save_best_only=True, save_weights_only=False)]\n",
    "tf.random.set_seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "lu = 100\n",
    "f = open('./ecc/arabidopsis/'+str(lu)+'/arabidopsis.csv')\n",
    "train_data = []\n",
    "label = []\n",
    "while(True):\n",
    "    tt = f.readline()[:-1]\n",
    "    if(len(tt) <= 1):\n",
    "        break\n",
    "    tt = tt.split(',')\n",
    "    dd = []\n",
    "    for j in tt[0].upper():\n",
    "        if(j == 'A'):\n",
    "            dd.append([0,0,0,1])\n",
    "        elif (j == 'G'):\n",
    "            dd.append([0,0,1,0])\n",
    "        elif (j == 'C'):\n",
    "            dd.append([0,1,0,0])\n",
    "        elif (j == 'T'):\n",
    "            dd.append([1,0,0,0])\n",
    "        else:\n",
    "            dd.append([0,0,0,0])\n",
    "    while(len(dd) < lu*2):\n",
    "        dd.append([0,0,0,0])\n",
    "    train_data.append(dd)\n",
    "\n",
    "    label.append(int(tt[1]))\n",
    "\n",
    "\n",
    "\n",
    "train_data = np.array(train_data)\n",
    "label = np.array(label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data,test_data,label,test_label=train_test_split(train_data,label,test_size=0.1,random_state=3753,stratify=label)\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_label = np.array(test_label)\n",
    "train_data = np.array(train_data)\n",
    "label = np.array(label)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inputs1 = Input(shape=(lu*2,4),name='1st_input')\n",
    "\n",
    "print(fn.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_mod = Conv1D(128,4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "\n",
    "x_mod = Conv1D(64,4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(x_mod)\n",
    "\n",
    "x_mod = MaxPooling1D(2)(x_mod)\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "# x_mod = BatchNormalization()(x_mod)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "co1 = Conv1D(x_mod.shape[1],4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "co2 = Conv1D(x_mod.shape[1],16, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co3 = Conv1D(247,6, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co4 = Conv1D(247,8, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co5 = Conv1D(247,10, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "pool1 = GlobalMaxPooling1D()(co1)\n",
    "pool2 = GlobalMaxPooling1D()(co2)\n",
    "# pool3 = GlobalMaxPooling1D()(co3)\n",
    "# pool4 = GlobalMaxPooling1D()(co4)\n",
    "# pool5 = GlobalMaxPooling1D()(co5)\n",
    "\n",
    "# 使用FPN结构合并多尺度特征\n",
    "fn = tf.add(pool1, pool2)\n",
    "fn = Reshape((x_mod.shape[1], -1))(fn)\n",
    "\n",
    "\n",
    "x_mod = Concatenate()([fn, x_mod])\n",
    "x_mod = LSTM(64,return_sequences=True)(x_mod)\n",
    "\n",
    "\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "\n",
    "\n",
    "x_mod = Flatten()(x_mod)\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "x_mod = Dense(64, activation='sigmoid')(x_mod)\n",
    "out = Dense(1, activation='sigmoid')(x_mod)\n",
    "\n",
    "model = Model(inputs=[inputs1], outputs=[out])#合成模型\n",
    "\n",
    "\n",
    "# 自定义损失函数\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # 交叉熵损失\n",
    "    cross_entropy_loss = binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # Hinge Cross-Entropy Loss\n",
    "    hinge_loss = tf.losses.hinge(y_true, y_pred)\n",
    "    \n",
    "    # 将两个损失相加\n",
    "    total_loss = 0.95*cross_entropy_loss + 0.05*hinge_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss=custom_loss,\n",
    "              optimizer=optimizers.Adam(learning_rate=0.001,beta_1=0.9, beta_2=0.888, epsilon=1e-08),metrics=['accuracy'],\n",
    "              )\n",
    "\n",
    "\n",
    "\n",
    "train_history = model.fit(train_data,\n",
    "                          label,\n",
    "                          validation_split=0.1,\n",
    "                          epochs=20,\n",
    "                          batch_size=64,\n",
    "                          verbose=1,\n",
    "                          shuffle=True,\n",
    "                          callbacks = callback4)\n",
    "\n",
    "\n",
    "model.load_weights('./model/bestmodel')\n",
    "pl = model.predict(test_data)\n",
    "p_test_label = []\n",
    "\n",
    "count = 0\n",
    "for i in range(len(pl)):\n",
    "    if(pl[i] >= 0.5):\n",
    "        p_test_label.append(1)\n",
    "    else:\n",
    "        p_test_label.append(0)\n",
    "# print(p_test_label)\n",
    "TP, FP, TN, FN, ACC, F1, MCC = comparison(p_test_label, test_label)\n",
    "print('PLeccNET')\n",
    "print('TP:', TP, 'FP:', FP, 'TN:', TN, 'FN:', FN)\n",
    "print('ACC:', ACC, 'F1:', F1, 'MCC:', MCC) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "386adf88-c8e5-4eb7-8e62-f190bf56a1e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35980, 300, 4)\n",
      "(3998, 300, 4)\n",
      "Epoch 1/20\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.5211 - accuracy: 0.7718INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 19s 29ms/step - loss: 0.5211 - accuracy: 0.7718 - val_loss: 0.4538 - val_accuracy: 0.8107 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "504/506 [============================>.] - ETA: 0s - loss: 0.3964 - accuracy: 0.8532INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 14s 27ms/step - loss: 0.3960 - accuracy: 0.8533 - val_loss: 0.3758 - val_accuracy: 0.8644 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "503/506 [============================>.] - ETA: 0s - loss: 0.3699 - accuracy: 0.8623INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 13s 26ms/step - loss: 0.3699 - accuracy: 0.8622 - val_loss: 0.3761 - val_accuracy: 0.8649 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "506/506 [==============================] - 9s 18ms/step - loss: 0.3536 - accuracy: 0.8705 - val_loss: 0.3786 - val_accuracy: 0.8641 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "506/506 [==============================] - 9s 18ms/step - loss: 0.3406 - accuracy: 0.8760 - val_loss: 0.4665 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.3407 - accuracy: 0.8786INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 14s 27ms/step - loss: 0.3407 - accuracy: 0.8786 - val_loss: 0.3646 - val_accuracy: 0.8716 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "506/506 [==============================] - 9s 18ms/step - loss: 0.3284 - accuracy: 0.8854 - val_loss: 0.3567 - val_accuracy: 0.8688 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "506/506 [==============================] - 9s 18ms/step - loss: 0.3198 - accuracy: 0.8895 - val_loss: 0.3729 - val_accuracy: 0.8680 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "506/506 [==============================] - 9s 18ms/step - loss: 0.3070 - accuracy: 0.8971 - val_loss: 0.3640 - val_accuracy: 0.8685 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.3182 - accuracy: 0.8913INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 14s 27ms/step - loss: 0.3182 - accuracy: 0.8913 - val_loss: 0.3478 - val_accuracy: 0.8785 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "506/506 [==============================] - 9s 18ms/step - loss: 0.2985 - accuracy: 0.9009 - val_loss: 0.3937 - val_accuracy: 0.8549 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "504/506 [============================>.] - ETA: 0s - loss: 0.2860 - accuracy: 0.9066INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 14s 27ms/step - loss: 0.2861 - accuracy: 0.9065 - val_loss: 0.3447 - val_accuracy: 0.8819 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "503/506 [============================>.] - ETA: 0s - loss: 0.2888 - accuracy: 0.9044INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 14s 27ms/step - loss: 0.2888 - accuracy: 0.9045 - val_loss: 0.3431 - val_accuracy: 0.8833 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "506/506 [==============================] - 9s 18ms/step - loss: 0.2796 - accuracy: 0.9097 - val_loss: 0.4470 - val_accuracy: 0.8346 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "506/506 [==============================] - 9s 18ms/step - loss: 0.2716 - accuracy: 0.9128 - val_loss: 0.4122 - val_accuracy: 0.8605 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "506/506 [==============================] - 9s 18ms/step - loss: 0.2708 - accuracy: 0.9134 - val_loss: 0.3820 - val_accuracy: 0.8602 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "506/506 [==============================] - 9s 18ms/step - loss: 0.2704 - accuracy: 0.9162 - val_loss: 0.3404 - val_accuracy: 0.8791 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "506/506 [==============================] - 9s 18ms/step - loss: 0.2612 - accuracy: 0.9187 - val_loss: 0.3486 - val_accuracy: 0.8816 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "506/506 [==============================] - 9s 18ms/step - loss: 0.2599 - accuracy: 0.9193 - val_loss: 0.3659 - val_accuracy: 0.8766 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "506/506 [==============================] - 9s 18ms/step - loss: 0.2598 - accuracy: 0.9189 - val_loss: 0.4141 - val_accuracy: 0.8541 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 10:16:57.871043: W tensorflow/core/util/tensor_slice_reader.cc:97] Could not open ./model/bestmodel: FAILED_PRECONDITION: model/bestmodel; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 1s 6ms/step\n",
      "PLeccNET\n",
      "TP: 1785 FP: 214 TN: 1722 FN: 277\n",
      "ACC: 0.8771885942971486 F1: 0.8790938192563408 MCC: 0.7547521078358185\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, MaxPool1D, Flatten, Dropout, Conv1D,Reshape,Input,AveragePooling1D,LSTM,Attention,MultiHeadAttention,MaxPooling1D,GlobalMaxPooling1D,Concatenate,BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers,Model,optimizers\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.losses import binary_crossentropy, hinge\n",
    "callback4 = [callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10,\n",
    "                                             verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0),\n",
    "                 callbacks.ModelCheckpoint(filepath=r'./model/bestmodel', monitor='val_accuracy', verbose=0,mode = 'max',\n",
    "                                           save_best_only=True, save_weights_only=False)]\n",
    "tf.random.set_seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "lu = 150\n",
    "f = open('./ecc/arabidopsis/'+str(lu)+'/arabidopsis.csv')\n",
    "train_data = []\n",
    "label = []\n",
    "while(True):\n",
    "    tt = f.readline()[:-1]\n",
    "    if(len(tt) <= 1):\n",
    "        break\n",
    "    tt = tt.split(',')\n",
    "    dd = []\n",
    "    for j in tt[0].upper():\n",
    "        if(j == 'A'):\n",
    "            dd.append([0,0,0,1])\n",
    "        elif (j == 'G'):\n",
    "            dd.append([0,0,1,0])\n",
    "        elif (j == 'C'):\n",
    "            dd.append([0,1,0,0])\n",
    "        elif (j == 'T'):\n",
    "            dd.append([1,0,0,0])\n",
    "        else:\n",
    "            dd.append([0,0,0,0])\n",
    "    while(len(dd) < lu*2):\n",
    "        dd.append([0,0,0,0])\n",
    "    train_data.append(dd)\n",
    "\n",
    "    label.append(int(tt[1]))\n",
    "\n",
    "\n",
    "\n",
    "train_data = np.array(train_data)\n",
    "label = np.array(label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data,test_data,label,test_label=train_test_split(train_data,label,test_size=0.1,random_state=3753,stratify=label)\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_label = np.array(test_label)\n",
    "train_data = np.array(train_data)\n",
    "label = np.array(label)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inputs1 = Input(shape=(lu*2,4),name='1st_input')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_mod = Conv1D(128,4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "\n",
    "x_mod = Conv1D(64,4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(x_mod)\n",
    "\n",
    "x_mod = MaxPooling1D(2)(x_mod)\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "# x_mod = BatchNormalization()(x_mod)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "co1 = Conv1D(x_mod.shape[1],4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "co2 = Conv1D(x_mod.shape[1],16, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co3 = Conv1D(247,6, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co4 = Conv1D(247,8, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co5 = Conv1D(247,10, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "pool1 = GlobalMaxPooling1D()(co1)\n",
    "pool2 = GlobalMaxPooling1D()(co2)\n",
    "# pool3 = GlobalMaxPooling1D()(co3)\n",
    "# pool4 = GlobalMaxPooling1D()(co4)\n",
    "# pool5 = GlobalMaxPooling1D()(co5)\n",
    "\n",
    "# 使用FPN结构合并多尺度特征\n",
    "fn = tf.add(pool1, pool2)\n",
    "fn = Reshape((x_mod.shape[1], -1))(fn)\n",
    "\n",
    "\n",
    "x_mod = Concatenate()([fn, x_mod])\n",
    "x_mod = LSTM(64,return_sequences=True)(x_mod)\n",
    "\n",
    "\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "\n",
    "\n",
    "x_mod = Flatten()(x_mod)\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "x_mod = Dense(64, activation='sigmoid')(x_mod)\n",
    "out = Dense(1, activation='sigmoid')(x_mod)\n",
    "\n",
    "model = Model(inputs=[inputs1], outputs=[out])#合成模型\n",
    "\n",
    "\n",
    "# 自定义损失函数\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # 交叉熵损失\n",
    "    cross_entropy_loss = binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # Hinge Cross-Entropy Loss\n",
    "    hinge_loss = tf.losses.hinge(y_true, y_pred)\n",
    "    \n",
    "    # 将两个损失相加\n",
    "    total_loss = 0.95*cross_entropy_loss + 0.05*hinge_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss=custom_loss,\n",
    "              optimizer=optimizers.Adam(learning_rate=0.001,beta_1=0.9, beta_2=0.888, epsilon=1e-08),metrics=['accuracy'],\n",
    "              )\n",
    "\n",
    "\n",
    "\n",
    "train_history = model.fit(train_data,\n",
    "                          label,\n",
    "                          validation_split=0.1,\n",
    "                          epochs=20,\n",
    "                          batch_size=64,\n",
    "                          verbose=1,\n",
    "                          shuffle=True,\n",
    "                          callbacks = callback4)\n",
    "\n",
    "\n",
    "model.load_weights('./model/bestmodel')\n",
    "pl = model.predict(test_data)\n",
    "p_test_label = []\n",
    "\n",
    "count = 0\n",
    "for i in range(len(pl)):\n",
    "    if(pl[i] >= 0.5):\n",
    "        p_test_label.append(1)\n",
    "    else:\n",
    "        p_test_label.append(0)\n",
    "# print(p_test_label)\n",
    "TP, FP, TN, FN, ACC, F1, MCC = comparison(p_test_label, test_label)\n",
    "print('PLeccNET')\n",
    "print('TP:', TP, 'FP:', FP, 'TN:', TN, 'FN:', FN)\n",
    "print('ACC:', ACC, 'F1:', F1, 'MCC:', MCC) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "510d4ab0-66ab-4669-bbe4-7d922c3300ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35980, 400, 4)\n",
      "(3998, 400, 4)\n",
      "Epoch 1/20\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.4826 - accuracy: 0.8019INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 21s 33ms/step - loss: 0.4826 - accuracy: 0.8019 - val_loss: 0.4123 - val_accuracy: 0.8658 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "505/506 [============================>.] - ETA: 0s - loss: 0.3880 - accuracy: 0.8597INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 15s 31ms/step - loss: 0.3878 - accuracy: 0.8599 - val_loss: 0.3434 - val_accuracy: 0.8822 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "506/506 [==============================] - 11s 22ms/step - loss: 0.3582 - accuracy: 0.8722 - val_loss: 0.3452 - val_accuracy: 0.8799 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "506/506 [==============================] - 11s 22ms/step - loss: 0.3333 - accuracy: 0.8848 - val_loss: 0.3421 - val_accuracy: 0.8785 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "505/506 [============================>.] - ETA: 0s - loss: 0.3140 - accuracy: 0.8934INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 16s 31ms/step - loss: 0.3141 - accuracy: 0.8934 - val_loss: 0.3064 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "506/506 [==============================] - 11s 22ms/step - loss: 0.3124 - accuracy: 0.8944 - val_loss: 0.3085 - val_accuracy: 0.8977 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "506/506 [==============================] - 11s 22ms/step - loss: 0.2985 - accuracy: 0.9011 - val_loss: 0.3111 - val_accuracy: 0.8955 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "506/506 [==============================] - 11s 22ms/step - loss: 0.2854 - accuracy: 0.9070 - val_loss: 0.3151 - val_accuracy: 0.8897 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "506/506 [==============================] - 11s 22ms/step - loss: 0.2728 - accuracy: 0.9143 - val_loss: 0.2965 - val_accuracy: 0.8966 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "505/506 [============================>.] - ETA: 0s - loss: 0.2884 - accuracy: 0.9063INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 15s 31ms/step - loss: 0.2883 - accuracy: 0.9063 - val_loss: 0.2957 - val_accuracy: 0.9011 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "506/506 [==============================] - 11s 22ms/step - loss: 0.2666 - accuracy: 0.9160 - val_loss: 0.3050 - val_accuracy: 0.8999 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "506/506 [==============================] - 11s 22ms/step - loss: 0.2580 - accuracy: 0.9195 - val_loss: 0.3257 - val_accuracy: 0.8880 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "505/506 [============================>.] - ETA: 0s - loss: 0.2601 - accuracy: 0.9184INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 15s 31ms/step - loss: 0.2600 - accuracy: 0.9185 - val_loss: 0.2911 - val_accuracy: 0.9049 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "506/506 [==============================] - 11s 22ms/step - loss: 0.2520 - accuracy: 0.9225 - val_loss: 0.3085 - val_accuracy: 0.8963 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "506/506 [==============================] - 11s 22ms/step - loss: 0.2477 - accuracy: 0.9254 - val_loss: 0.3404 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "506/506 [==============================] - 11s 22ms/step - loss: 0.2415 - accuracy: 0.9271 - val_loss: 0.3864 - val_accuracy: 0.8616 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "506/506 [==============================] - 11s 22ms/step - loss: 0.2381 - accuracy: 0.9292 - val_loss: 0.4303 - val_accuracy: 0.8524 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "506/506 [==============================] - 11s 22ms/step - loss: 0.2305 - accuracy: 0.9324 - val_loss: 0.2942 - val_accuracy: 0.9036 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "506/506 [==============================] - 11s 22ms/step - loss: 0.2253 - accuracy: 0.9348 - val_loss: 0.3016 - val_accuracy: 0.9030 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "506/506 [==============================] - 11s 22ms/step - loss: 0.2195 - accuracy: 0.9363 - val_loss: 0.4541 - val_accuracy: 0.8469 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 10:22:14.120209: W tensorflow/core/util/tensor_slice_reader.cc:97] Could not open ./model/bestmodel: FAILED_PRECONDITION: model/bestmodel; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 1s 7ms/step\n",
      "PLeccNET\n",
      "TP: 1702 FP: 297 TN: 1878 FN: 121\n",
      "ACC: 0.8954477238619309 F1: 0.8906331763474621 MCC: 0.7939787971830401\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, MaxPool1D, Flatten, Dropout, Conv1D,Reshape,Input,AveragePooling1D,LSTM,Attention,MultiHeadAttention,MaxPooling1D,GlobalMaxPooling1D,Concatenate,BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers,Model,optimizers\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.losses import binary_crossentropy, hinge\n",
    "callback4 = [callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10,\n",
    "                                             verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0),\n",
    "                 callbacks.ModelCheckpoint(filepath=r'./model/bestmodel', monitor='val_accuracy', verbose=0,mode = 'max',\n",
    "                                           save_best_only=True, save_weights_only=False)]\n",
    "tf.random.set_seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "lu = 200\n",
    "f = open('./ecc/arabidopsis/'+str(lu)+'/arabidopsis.csv')\n",
    "train_data = []\n",
    "label = []\n",
    "while(True):\n",
    "    tt = f.readline()[:-1]\n",
    "    if(len(tt) <= 1):\n",
    "        break\n",
    "    tt = tt.split(',')\n",
    "    dd = []\n",
    "    for j in tt[0].upper():\n",
    "        if(j == 'A'):\n",
    "            dd.append([0,0,0,1])\n",
    "        elif (j == 'G'):\n",
    "            dd.append([0,0,1,0])\n",
    "        elif (j == 'C'):\n",
    "            dd.append([0,1,0,0])\n",
    "        elif (j == 'T'):\n",
    "            dd.append([1,0,0,0])\n",
    "        else:\n",
    "            dd.append([0,0,0,0])\n",
    "    while(len(dd) < lu*2):\n",
    "        dd.append([0,0,0,0])\n",
    "    train_data.append(dd)\n",
    "\n",
    "    label.append(int(tt[1]))\n",
    "\n",
    "\n",
    "\n",
    "train_data = np.array(train_data)\n",
    "label = np.array(label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data,test_data,label,test_label=train_test_split(train_data,label,test_size=0.1,random_state=3753,stratify=label)\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_label = np.array(test_label)\n",
    "train_data = np.array(train_data)\n",
    "label = np.array(label)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inputs1 = Input(shape=(lu*2,4),name='1st_input')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_mod = Conv1D(128,4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "\n",
    "x_mod = Conv1D(64,4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(x_mod)\n",
    "\n",
    "x_mod = MaxPooling1D(2)(x_mod)\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "# x_mod = BatchNormalization()(x_mod)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "co1 = Conv1D(x_mod.shape[1],4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "co2 = Conv1D(x_mod.shape[1],16, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co3 = Conv1D(247,6, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co4 = Conv1D(247,8, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co5 = Conv1D(247,10, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "pool1 = GlobalMaxPooling1D()(co1)\n",
    "pool2 = GlobalMaxPooling1D()(co2)\n",
    "# pool3 = GlobalMaxPooling1D()(co3)\n",
    "# pool4 = GlobalMaxPooling1D()(co4)\n",
    "# pool5 = GlobalMaxPooling1D()(co5)\n",
    "\n",
    "# 使用FPN结构合并多尺度特征\n",
    "fn = tf.add(pool1, pool2)\n",
    "fn = Reshape((x_mod.shape[1], -1))(fn)\n",
    "\n",
    "\n",
    "x_mod = Concatenate()([fn, x_mod])\n",
    "x_mod = LSTM(64,return_sequences=True)(x_mod)\n",
    "\n",
    "\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "\n",
    "\n",
    "x_mod = Flatten()(x_mod)\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "x_mod = Dense(64, activation='sigmoid')(x_mod)\n",
    "out = Dense(1, activation='sigmoid')(x_mod)\n",
    "\n",
    "model = Model(inputs=[inputs1], outputs=[out])#合成模型\n",
    "\n",
    "\n",
    "# 自定义损失函数\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # 交叉熵损失\n",
    "    cross_entropy_loss = binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # Hinge Cross-Entropy Loss\n",
    "    hinge_loss = tf.losses.hinge(y_true, y_pred)\n",
    "    \n",
    "    # 将两个损失相加\n",
    "    total_loss = 0.95*cross_entropy_loss + 0.05*hinge_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss=custom_loss,\n",
    "              optimizer=optimizers.Adam(learning_rate=0.001,beta_1=0.9, beta_2=0.888, epsilon=1e-08),metrics=['accuracy'],\n",
    "              )\n",
    "\n",
    "\n",
    "\n",
    "train_history = model.fit(train_data,\n",
    "                          label,\n",
    "                          validation_split=0.1,\n",
    "                          epochs=20,\n",
    "                          batch_size=64,\n",
    "                          verbose=1,\n",
    "                          shuffle=True,\n",
    "                          callbacks = callback4)\n",
    "\n",
    "\n",
    "model.load_weights('./model/bestmodel')\n",
    "pl = model.predict(test_data)\n",
    "p_test_label = []\n",
    "\n",
    "count = 0\n",
    "for i in range(len(pl)):\n",
    "    if(pl[i] >= 0.5):\n",
    "        p_test_label.append(1)\n",
    "    else:\n",
    "        p_test_label.append(0)\n",
    "# print(p_test_label)\n",
    "TP, FP, TN, FN, ACC, F1, MCC = comparison(p_test_label, test_label)\n",
    "print('PLeccNET')\n",
    "print('TP:', TP, 'FP:', FP, 'TN:', TN, 'FN:', FN)\n",
    "print('ACC:', ACC, 'F1:', F1, 'MCC:', MCC) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc0ad98b-2470-412b-9bf3-169e08580dd3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35980, 600, 4)\n",
      "(3998, 600, 4)\n",
      "Epoch 1/20\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.4469 - accuracy: 0.8090INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 27s 44ms/step - loss: 0.4469 - accuracy: 0.8090 - val_loss: 0.3179 - val_accuracy: 0.8936 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "505/506 [============================>.] - ETA: 0s - loss: 0.2892 - accuracy: 0.9004INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 22s 43ms/step - loss: 0.2890 - accuracy: 0.9005 - val_loss: 0.2741 - val_accuracy: 0.8986 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "505/506 [============================>.] - ETA: 0s - loss: 0.2441 - accuracy: 0.9189INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 22s 43ms/step - loss: 0.2441 - accuracy: 0.9189 - val_loss: 0.2619 - val_accuracy: 0.9072 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "505/506 [============================>.] - ETA: 0s - loss: 0.2343 - accuracy: 0.9233INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 22s 43ms/step - loss: 0.2344 - accuracy: 0.9232 - val_loss: 0.2647 - val_accuracy: 0.9111 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "506/506 [==============================] - 17s 33ms/step - loss: 0.2143 - accuracy: 0.9337 - val_loss: 0.4166 - val_accuracy: 0.8588 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "506/506 [==============================] - 17s 33ms/step - loss: 0.2087 - accuracy: 0.9362 - val_loss: 0.3366 - val_accuracy: 0.8813 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "505/506 [============================>.] - ETA: 0s - loss: 0.2006 - accuracy: 0.9430INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 21s 42ms/step - loss: 0.2009 - accuracy: 0.9429 - val_loss: 0.2717 - val_accuracy: 0.9188 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "506/506 [==============================] - 17s 33ms/step - loss: 0.1933 - accuracy: 0.9463 - val_loss: 0.2843 - val_accuracy: 0.9138 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "506/506 [==============================] - 17s 33ms/step - loss: 0.1914 - accuracy: 0.9480 - val_loss: 0.3015 - val_accuracy: 0.9038 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "506/506 [==============================] - 17s 33ms/step - loss: 0.1948 - accuracy: 0.9463 - val_loss: 0.3145 - val_accuracy: 0.8922 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "506/506 [==============================] - 17s 33ms/step - loss: 0.2036 - accuracy: 0.9430 - val_loss: 0.3810 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "506/506 [==============================] - 17s 33ms/step - loss: 0.1844 - accuracy: 0.9518 - val_loss: 0.3320 - val_accuracy: 0.8961 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "506/506 [==============================] - 17s 33ms/step - loss: 0.1770 - accuracy: 0.9568 - val_loss: 0.3076 - val_accuracy: 0.8988 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "506/506 [==============================] - 17s 33ms/step - loss: 0.1260 - accuracy: 0.9791 - val_loss: 0.3313 - val_accuracy: 0.9138 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "506/506 [==============================] - 17s 33ms/step - loss: 0.1110 - accuracy: 0.9849 - val_loss: 0.3688 - val_accuracy: 0.9144 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "506/506 [==============================] - 17s 33ms/step - loss: 0.1003 - accuracy: 0.9881 - val_loss: 0.4109 - val_accuracy: 0.9113 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "506/506 [==============================] - 17s 33ms/step - loss: 0.0937 - accuracy: 0.9906 - val_loss: 0.4711 - val_accuracy: 0.9074 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "506/506 [==============================] - 17s 33ms/step - loss: 0.0886 - accuracy: 0.9916 - val_loss: 0.4807 - val_accuracy: 0.9041 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "506/506 [==============================] - 17s 33ms/step - loss: 0.0845 - accuracy: 0.9930 - val_loss: 0.4692 - val_accuracy: 0.9069 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "506/506 [==============================] - 17s 33ms/step - loss: 0.0817 - accuracy: 0.9937 - val_loss: 0.4780 - val_accuracy: 0.9063 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 10:30:16.822609: W tensorflow/core/util/tensor_slice_reader.cc:97] Could not open ./model/bestmodel: FAILED_PRECONDITION: model/bestmodel; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 2s 8ms/step\n",
      "PLeccNET\n",
      "TP: 1825 FP: 174 TN: 1825 FN: 174\n",
      "ACC: 0.9129564782391195 F1: 0.9129564782391195 MCC: 0.8259129564782391\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, MaxPool1D, Flatten, Dropout, Conv1D,Reshape,Input,AveragePooling1D,LSTM,Attention,MultiHeadAttention,MaxPooling1D,GlobalMaxPooling1D,Concatenate,BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers,Model,optimizers\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.losses import binary_crossentropy, hinge\n",
    "callback4 = [callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10,\n",
    "                                             verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0),\n",
    "                 callbacks.ModelCheckpoint(filepath=r'./model/bestmodel', monitor='val_accuracy', verbose=0,mode = 'max',\n",
    "                                           save_best_only=True, save_weights_only=False)]\n",
    "tf.random.set_seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "lu = 300\n",
    "f = open('./ecc/arabidopsis/'+str(lu)+'/arabidopsis.csv')\n",
    "train_data = []\n",
    "label = []\n",
    "while(True):\n",
    "    tt = f.readline()[:-1]\n",
    "    if(len(tt) <= 1):\n",
    "        break\n",
    "    tt = tt.split(',')\n",
    "    dd = []\n",
    "    for j in tt[0].upper():\n",
    "        if(j == 'A'):\n",
    "            dd.append([0,0,0,1])\n",
    "        elif (j == 'G'):\n",
    "            dd.append([0,0,1,0])\n",
    "        elif (j == 'C'):\n",
    "            dd.append([0,1,0,0])\n",
    "        elif (j == 'T'):\n",
    "            dd.append([1,0,0,0])\n",
    "        else:\n",
    "            dd.append([0,0,0,0])\n",
    "    while(len(dd) < lu*2):\n",
    "        dd.append([0,0,0,0])\n",
    "    train_data.append(dd)\n",
    "\n",
    "    label.append(int(tt[1]))\n",
    "\n",
    "\n",
    "\n",
    "train_data = np.array(train_data)\n",
    "label = np.array(label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data,test_data,label,test_label=train_test_split(train_data,label,test_size=0.1,random_state=3753,stratify=label)\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_label = np.array(test_label)\n",
    "train_data = np.array(train_data)\n",
    "label = np.array(label)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inputs1 = Input(shape=(lu*2,4),name='1st_input')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_mod = Conv1D(128,4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "\n",
    "x_mod = Conv1D(64,4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(x_mod)\n",
    "\n",
    "x_mod = MaxPooling1D(2)(x_mod)\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "# x_mod = BatchNormalization()(x_mod)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "co1 = Conv1D(x_mod.shape[1],4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "co2 = Conv1D(x_mod.shape[1],16, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co3 = Conv1D(247,6, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co4 = Conv1D(247,8, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co5 = Conv1D(247,10, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "pool1 = GlobalMaxPooling1D()(co1)\n",
    "pool2 = GlobalMaxPooling1D()(co2)\n",
    "# pool3 = GlobalMaxPooling1D()(co3)\n",
    "# pool4 = GlobalMaxPooling1D()(co4)\n",
    "# pool5 = GlobalMaxPooling1D()(co5)\n",
    "\n",
    "# 使用FPN结构合并多尺度特征\n",
    "fn = tf.add(pool1, pool2)\n",
    "fn = Reshape((x_mod.shape[1], -1))(fn)\n",
    "\n",
    "\n",
    "x_mod = Concatenate()([fn, x_mod])\n",
    "x_mod = LSTM(64,return_sequences=True)(x_mod)\n",
    "\n",
    "\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "\n",
    "\n",
    "x_mod = Flatten()(x_mod)\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "x_mod = Dense(64, activation='sigmoid')(x_mod)\n",
    "out = Dense(1, activation='sigmoid')(x_mod)\n",
    "\n",
    "model = Model(inputs=[inputs1], outputs=[out])#合成模型\n",
    "\n",
    "\n",
    "# 自定义损失函数\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # 交叉熵损失\n",
    "    cross_entropy_loss = binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # Hinge Cross-Entropy Loss\n",
    "    hinge_loss = tf.losses.hinge(y_true, y_pred)\n",
    "    \n",
    "    # 将两个损失相加\n",
    "    total_loss = 0.95*cross_entropy_loss + 0.05*hinge_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss=custom_loss,\n",
    "              optimizer=optimizers.Adam(learning_rate=0.001,beta_1=0.9, beta_2=0.888, epsilon=1e-08),metrics=['accuracy'],\n",
    "              )\n",
    "\n",
    "\n",
    "\n",
    "train_history = model.fit(train_data,\n",
    "                          label,\n",
    "                          validation_split=0.1,\n",
    "                          epochs=20,\n",
    "                          batch_size=64,\n",
    "                          verbose=1,\n",
    "                          shuffle=True,\n",
    "                          callbacks = callback4)\n",
    "\n",
    "\n",
    "model.load_weights('./model/bestmodel')\n",
    "pl = model.predict(test_data)\n",
    "p_test_label = []\n",
    "\n",
    "count = 0\n",
    "for i in range(len(pl)):\n",
    "    if(pl[i] >= 0.5):\n",
    "        p_test_label.append(1)\n",
    "    else:\n",
    "        p_test_label.append(0)\n",
    "# print(p_test_label)\n",
    "TP, FP, TN, FN, ACC, F1, MCC = comparison(p_test_label, test_label)\n",
    "print('PLeccNET')\n",
    "print('TP:', TP, 'FP:', FP, 'TN:', TN, 'FN:', FN)\n",
    "print('ACC:', ACC, 'F1:', F1, 'MCC:', MCC) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "952f818e-2906-4b91-a829-c8125978f1ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35980, 700, 4)\n",
      "(3998, 700, 4)\n",
      "Epoch 1/20\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.4533 - accuracy: 0.8105INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 30s 51ms/step - loss: 0.4533 - accuracy: 0.8105 - val_loss: 0.3720 - val_accuracy: 0.8602 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "505/506 [============================>.] - ETA: 0s - loss: 0.3349 - accuracy: 0.8844INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 25s 49ms/step - loss: 0.3348 - accuracy: 0.8844 - val_loss: 0.3292 - val_accuracy: 0.8877 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "506/506 [==============================] - 20s 39ms/step - loss: 0.6729 - accuracy: 0.6474 - val_loss: 0.7089 - val_accuracy: 0.4956 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "506/506 [==============================] - 20s 40ms/step - loss: 0.7090 - accuracy: 0.5019 - val_loss: 0.7082 - val_accuracy: 0.5044 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "506/506 [==============================] - 20s 39ms/step - loss: 0.7090 - accuracy: 0.4969 - val_loss: 0.7083 - val_accuracy: 0.4956 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "506/506 [==============================] - 20s 39ms/step - loss: 0.7092 - accuracy: 0.4993 - val_loss: 0.7088 - val_accuracy: 0.5044 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "506/506 [==============================] - 20s 39ms/step - loss: 0.7092 - accuracy: 0.5001 - val_loss: 0.7089 - val_accuracy: 0.4956 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "506/506 [==============================] - 20s 39ms/step - loss: 0.7092 - accuracy: 0.5001 - val_loss: 0.7083 - val_accuracy: 0.5044 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "506/506 [==============================] - 20s 39ms/step - loss: 0.7093 - accuracy: 0.5008 - val_loss: 0.7095 - val_accuracy: 0.5044 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "506/506 [==============================] - 20s 39ms/step - loss: 0.7089 - accuracy: 0.5025 - val_loss: 0.7096 - val_accuracy: 0.4956 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "506/506 [==============================] - 20s 39ms/step - loss: 0.7090 - accuracy: 0.5045 - val_loss: 0.7084 - val_accuracy: 0.4956 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "506/506 [==============================] - 20s 39ms/step - loss: 0.7090 - accuracy: 0.4977 - val_loss: 0.7097 - val_accuracy: 0.5044 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "506/506 [==============================] - 20s 39ms/step - loss: 0.7088 - accuracy: 0.4970 - val_loss: 0.7083 - val_accuracy: 0.4956 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "506/506 [==============================] - 20s 39ms/step - loss: 0.7086 - accuracy: 0.5016 - val_loss: 0.7082 - val_accuracy: 0.5044 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "506/506 [==============================] - 20s 39ms/step - loss: 0.7086 - accuracy: 0.4986 - val_loss: 0.7083 - val_accuracy: 0.4956 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "506/506 [==============================] - 20s 39ms/step - loss: 0.7086 - accuracy: 0.4975 - val_loss: 0.7085 - val_accuracy: 0.4956 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "506/506 [==============================] - 20s 39ms/step - loss: 0.7086 - accuracy: 0.4996 - val_loss: 0.7085 - val_accuracy: 0.4956 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "506/506 [==============================] - 20s 39ms/step - loss: 0.7086 - accuracy: 0.4999 - val_loss: 0.7082 - val_accuracy: 0.5044 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "506/506 [==============================] - 20s 39ms/step - loss: 0.7086 - accuracy: 0.4939 - val_loss: 0.7083 - val_accuracy: 0.5044 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "506/506 [==============================] - 20s 39ms/step - loss: 0.7086 - accuracy: 0.4952 - val_loss: 0.7086 - val_accuracy: 0.4956 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 10:39:08.564874: W tensorflow/core/util/tensor_slice_reader.cc:97] Could not open ./model/bestmodel: FAILED_PRECONDITION: model/bestmodel; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 2s 9ms/step\n",
      "PLeccNET\n",
      "TP: 1602 FP: 397 TN: 1935 FN: 64\n",
      "ACC: 0.8846923461730866 F1: 0.8742155525238745 MCC: 0.7802873458420673\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, MaxPool1D, Flatten, Dropout, Conv1D,Reshape,Input,AveragePooling1D,LSTM,Attention,MultiHeadAttention,MaxPooling1D,GlobalMaxPooling1D,Concatenate,BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers,Model,optimizers\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.losses import binary_crossentropy, hinge\n",
    "callback4 = [callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10,\n",
    "                                             verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0),\n",
    "                 callbacks.ModelCheckpoint(filepath=r'./model/bestmodel', monitor='val_accuracy', verbose=0,mode = 'max',\n",
    "                                           save_best_only=True, save_weights_only=False)]\n",
    "tf.random.set_seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "lu = 350\n",
    "f = open('./ecc/arabidopsis/'+str(lu)+'/arabidopsis.csv')\n",
    "train_data = []\n",
    "label = []\n",
    "while(True):\n",
    "    tt = f.readline()[:-1]\n",
    "    if(len(tt) <= 1):\n",
    "        break\n",
    "    tt = tt.split(',')\n",
    "    dd = []\n",
    "    for j in tt[0].upper():\n",
    "        if(j == 'A'):\n",
    "            dd.append([0,0,0,1])\n",
    "        elif (j == 'G'):\n",
    "            dd.append([0,0,1,0])\n",
    "        elif (j == 'C'):\n",
    "            dd.append([0,1,0,0])\n",
    "        elif (j == 'T'):\n",
    "            dd.append([1,0,0,0])\n",
    "        else:\n",
    "            dd.append([0,0,0,0])\n",
    "    while(len(dd) < lu*2):\n",
    "        dd.append([0,0,0,0])\n",
    "    train_data.append(dd)\n",
    "\n",
    "    label.append(int(tt[1]))\n",
    "\n",
    "\n",
    "\n",
    "train_data = np.array(train_data)\n",
    "label = np.array(label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data,test_data,label,test_label=train_test_split(train_data,label,test_size=0.1,random_state=3753,stratify=label)\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_label = np.array(test_label)\n",
    "train_data = np.array(train_data)\n",
    "label = np.array(label)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inputs1 = Input(shape=(lu*2,4),name='1st_input')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_mod = Conv1D(128,4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "\n",
    "x_mod = Conv1D(64,4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(x_mod)\n",
    "\n",
    "x_mod = MaxPooling1D(2)(x_mod)\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "# x_mod = BatchNormalization()(x_mod)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "co1 = Conv1D(x_mod.shape[1],4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "co2 = Conv1D(x_mod.shape[1],16, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co3 = Conv1D(247,6, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co4 = Conv1D(247,8, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co5 = Conv1D(247,10, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "pool1 = GlobalMaxPooling1D()(co1)\n",
    "pool2 = GlobalMaxPooling1D()(co2)\n",
    "# pool3 = GlobalMaxPooling1D()(co3)\n",
    "# pool4 = GlobalMaxPooling1D()(co4)\n",
    "# pool5 = GlobalMaxPooling1D()(co5)\n",
    "\n",
    "# 使用FPN结构合并多尺度特征\n",
    "fn = tf.add(pool1, pool2)\n",
    "fn = Reshape((x_mod.shape[1], -1))(fn)\n",
    "\n",
    "\n",
    "x_mod = Concatenate()([fn, x_mod])\n",
    "x_mod = LSTM(64,return_sequences=True)(x_mod)\n",
    "\n",
    "\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "\n",
    "\n",
    "x_mod = Flatten()(x_mod)\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "x_mod = Dense(64, activation='sigmoid')(x_mod)\n",
    "out = Dense(1, activation='sigmoid')(x_mod)\n",
    "\n",
    "model = Model(inputs=[inputs1], outputs=[out])#合成模型\n",
    "\n",
    "\n",
    "# 自定义损失函数\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # 交叉熵损失\n",
    "    cross_entropy_loss = binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # Hinge Cross-Entropy Loss\n",
    "    hinge_loss = tf.losses.hinge(y_true, y_pred)\n",
    "    \n",
    "    # 将两个损失相加\n",
    "    total_loss = 0.95*cross_entropy_loss + 0.05*hinge_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss=custom_loss,\n",
    "              optimizer=optimizers.Adam(learning_rate=0.001,beta_1=0.9, beta_2=0.888, epsilon=1e-08),metrics=['accuracy'],\n",
    "              )\n",
    "\n",
    "\n",
    "\n",
    "train_history = model.fit(train_data,\n",
    "                          label,\n",
    "                          validation_split=0.1,\n",
    "                          epochs=20,\n",
    "                          batch_size=64,\n",
    "                          verbose=1,\n",
    "                          shuffle=True,\n",
    "                          callbacks = callback4)\n",
    "\n",
    "\n",
    "model.load_weights('./model/bestmodel')\n",
    "pl = model.predict(test_data)\n",
    "p_test_label = []\n",
    "\n",
    "count = 0\n",
    "for i in range(len(pl)):\n",
    "    if(pl[i] >= 0.5):\n",
    "        p_test_label.append(1)\n",
    "    else:\n",
    "        p_test_label.append(0)\n",
    "# print(p_test_label)\n",
    "TP, FP, TN, FN, ACC, F1, MCC = comparison(p_test_label, test_label)\n",
    "print('PLeccNET')\n",
    "print('TP:', TP, 'FP:', FP, 'TN:', TN, 'FN:', FN)\n",
    "print('ACC:', ACC, 'F1:', F1, 'MCC:', MCC) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c92ebf4-8973-4460-b34f-5f41340c8def",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35980, 800, 4)\n",
      "(3998, 800, 4)\n",
      "Epoch 1/20\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.4391 - accuracy: 0.8154INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 34s 57ms/step - loss: 0.4391 - accuracy: 0.8154 - val_loss: 0.2976 - val_accuracy: 0.9080 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "506/506 [==============================] - 23s 46ms/step - loss: 0.7927 - accuracy: 0.6190 - val_loss: 0.7082 - val_accuracy: 0.5044 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "506/506 [==============================] - 24s 47ms/step - loss: 0.7090 - accuracy: 0.5016 - val_loss: 0.7087 - val_accuracy: 0.4956 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "506/506 [==============================] - 23s 46ms/step - loss: 0.7090 - accuracy: 0.5021 - val_loss: 0.7082 - val_accuracy: 0.5044 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "506/506 [==============================] - 23s 46ms/step - loss: 0.7092 - accuracy: 0.5001 - val_loss: 0.7089 - val_accuracy: 0.4956 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "506/506 [==============================] - 23s 46ms/step - loss: 0.7092 - accuracy: 0.5001 - val_loss: 0.7083 - val_accuracy: 0.5044 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "506/506 [==============================] - 23s 46ms/step - loss: 0.7093 - accuracy: 0.5008 - val_loss: 0.7095 - val_accuracy: 0.5044 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "506/506 [==============================] - 23s 46ms/step - loss: 0.7089 - accuracy: 0.5025 - val_loss: 0.7096 - val_accuracy: 0.4956 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "506/506 [==============================] - 23s 46ms/step - loss: 0.7090 - accuracy: 0.5045 - val_loss: 0.7084 - val_accuracy: 0.4956 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "506/506 [==============================] - 23s 46ms/step - loss: 0.7086 - accuracy: 0.4977 - val_loss: 0.7083 - val_accuracy: 0.5044 - lr: 1.0000e-04\n",
      "Epoch 13/20\n",
      "506/506 [==============================] - 23s 46ms/step - loss: 0.7086 - accuracy: 0.4981 - val_loss: 0.7083 - val_accuracy: 0.4956 - lr: 1.0000e-04\n",
      "Epoch 14/20\n",
      "506/506 [==============================] - 23s 46ms/step - loss: 0.7086 - accuracy: 0.5016 - val_loss: 0.7082 - val_accuracy: 0.5044 - lr: 1.0000e-04\n",
      "Epoch 15/20\n",
      "506/506 [==============================] - 23s 46ms/step - loss: 0.7086 - accuracy: 0.4986 - val_loss: 0.7083 - val_accuracy: 0.4956 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "506/506 [==============================] - 23s 46ms/step - loss: 0.7086 - accuracy: 0.4975 - val_loss: 0.7085 - val_accuracy: 0.4956 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "506/506 [==============================] - 23s 46ms/step - loss: 0.7086 - accuracy: 0.4996 - val_loss: 0.7085 - val_accuracy: 0.4956 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "506/506 [==============================] - 23s 46ms/step - loss: 0.7086 - accuracy: 0.4999 - val_loss: 0.7082 - val_accuracy: 0.5044 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "506/506 [==============================] - 23s 46ms/step - loss: 0.7086 - accuracy: 0.4939 - val_loss: 0.7083 - val_accuracy: 0.5044 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "506/506 [==============================] - 23s 46ms/step - loss: 0.7086 - accuracy: 0.4952 - val_loss: 0.7086 - val_accuracy: 0.4956 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 10:58:08.201382: W tensorflow/core/util/tensor_slice_reader.cc:97] Could not open ./model/bestmodel: FAILED_PRECONDITION: model/bestmodel; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 2s 11ms/step\n",
      "PLeccNET\n",
      "TP: 1824 FP: 175 TN: 1777 FN: 222\n",
      "ACC: 0.9007003501750875 F1: 0.9018541409147095 MCC: 0.8016223004514819\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, MaxPool1D, Flatten, Dropout, Conv1D,Reshape,Input,AveragePooling1D,LSTM,Attention,MultiHeadAttention,MaxPooling1D,GlobalMaxPooling1D,Concatenate,BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers,Model,optimizers\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.losses import binary_crossentropy, hinge\n",
    "callback4 = [callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10,\n",
    "                                             verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0),\n",
    "                 callbacks.ModelCheckpoint(filepath=r'./model/bestmodel', monitor='val_accuracy', verbose=0,mode = 'max',\n",
    "                                           save_best_only=True, save_weights_only=False)]\n",
    "tf.random.set_seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "lu = 400\n",
    "f = open('./ecc/arabidopsis/'+str(lu)+'/arabidopsis.csv')\n",
    "train_data = []\n",
    "label = []\n",
    "while(True):\n",
    "    tt = f.readline()[:-1]\n",
    "    if(len(tt) <= 1):\n",
    "        break\n",
    "    tt = tt.split(',')\n",
    "    dd = []\n",
    "    for j in tt[0].upper():\n",
    "        if(j == 'A'):\n",
    "            dd.append([0,0,0,1])\n",
    "        elif (j == 'G'):\n",
    "            dd.append([0,0,1,0])\n",
    "        elif (j == 'C'):\n",
    "            dd.append([0,1,0,0])\n",
    "        elif (j == 'T'):\n",
    "            dd.append([1,0,0,0])\n",
    "        else:\n",
    "            dd.append([0,0,0,0])\n",
    "    while(len(dd) < lu*2):\n",
    "        dd.append([0,0,0,0])\n",
    "    train_data.append(dd)\n",
    "\n",
    "    label.append(int(tt[1]))\n",
    "\n",
    "\n",
    "\n",
    "train_data = np.array(train_data)\n",
    "label = np.array(label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data,test_data,label,test_label=train_test_split(train_data,label,test_size=0.1,random_state=3753,stratify=label)\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_label = np.array(test_label)\n",
    "train_data = np.array(train_data)\n",
    "label = np.array(label)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inputs1 = Input(shape=(lu*2,4),name='1st_input')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_mod = Conv1D(128,4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "\n",
    "x_mod = Conv1D(64,4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(x_mod)\n",
    "\n",
    "x_mod = MaxPooling1D(2)(x_mod)\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "# x_mod = BatchNormalization()(x_mod)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "co1 = Conv1D(x_mod.shape[1],4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "co2 = Conv1D(x_mod.shape[1],16, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co3 = Conv1D(247,6, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co4 = Conv1D(247,8, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co5 = Conv1D(247,10, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "pool1 = GlobalMaxPooling1D()(co1)\n",
    "pool2 = GlobalMaxPooling1D()(co2)\n",
    "# pool3 = GlobalMaxPooling1D()(co3)\n",
    "# pool4 = GlobalMaxPooling1D()(co4)\n",
    "# pool5 = GlobalMaxPooling1D()(co5)\n",
    "\n",
    "# 使用FPN结构合并多尺度特征\n",
    "fn = tf.add(pool1, pool2)\n",
    "fn = Reshape((x_mod.shape[1], -1))(fn)\n",
    "\n",
    "\n",
    "x_mod = Concatenate()([fn, x_mod])\n",
    "x_mod = LSTM(64,return_sequences=True)(x_mod)\n",
    "\n",
    "\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "\n",
    "\n",
    "x_mod = Flatten()(x_mod)\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "x_mod = Dense(64, activation='sigmoid')(x_mod)\n",
    "out = Dense(1, activation='sigmoid')(x_mod)\n",
    "\n",
    "model = Model(inputs=[inputs1], outputs=[out])#合成模型\n",
    "\n",
    "\n",
    "# 自定义损失函数\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # 交叉熵损失\n",
    "    cross_entropy_loss = binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # Hinge Cross-Entropy Loss\n",
    "    hinge_loss = tf.losses.hinge(y_true, y_pred)\n",
    "    \n",
    "    # 将两个损失相加\n",
    "    total_loss = 0.95*cross_entropy_loss + 0.05*hinge_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss=custom_loss,\n",
    "              optimizer=optimizers.Adam(learning_rate=0.001,beta_1=0.9, beta_2=0.888, epsilon=1e-08),metrics=['accuracy'],\n",
    "              )\n",
    "\n",
    "\n",
    "\n",
    "train_history = model.fit(train_data,\n",
    "                          label,\n",
    "                          validation_split=0.1,\n",
    "                          epochs=20,\n",
    "                          batch_size=64,\n",
    "                          verbose=1,\n",
    "                          shuffle=True,\n",
    "                          callbacks = callback4)\n",
    "\n",
    "\n",
    "model.load_weights('./model/bestmodel')\n",
    "pl = model.predict(test_data)\n",
    "p_test_label = []\n",
    "\n",
    "count = 0\n",
    "for i in range(len(pl)):\n",
    "    if(pl[i] >= 0.5):\n",
    "        p_test_label.append(1)\n",
    "    else:\n",
    "        p_test_label.append(0)\n",
    "# print(p_test_label)\n",
    "TP, FP, TN, FN, ACC, F1, MCC = comparison(p_test_label, test_label)\n",
    "print('PLeccNET')\n",
    "print('TP:', TP, 'FP:', FP, 'TN:', TN, 'FN:', FN)\n",
    "print('ACC:', ACC, 'F1:', F1, 'MCC:', MCC) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8531ccf2-7186-4989-a405-9204a16ad85e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35980, 900, 4)\n",
      "(3998, 900, 4)\n",
      "Epoch 1/20\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.4415 - accuracy: 0.8142INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 36s 64ms/step - loss: 0.4415 - accuracy: 0.8142 - val_loss: 0.3148 - val_accuracy: 0.8908 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.2990 - accuracy: 0.8978INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 32s 63ms/step - loss: 0.2990 - accuracy: 0.8978 - val_loss: 0.2503 - val_accuracy: 0.9197 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "506/506 [==============================] - 27s 53ms/step - loss: 0.2683 - accuracy: 0.9099 - val_loss: 0.3280 - val_accuracy: 0.8766 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.2427 - accuracy: 0.9220INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 32s 62ms/step - loss: 0.2427 - accuracy: 0.9220 - val_loss: 0.2394 - val_accuracy: 0.9261 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "506/506 [==============================] - 27s 53ms/step - loss: 0.3291 - accuracy: 0.8450 - val_loss: 0.7224 - val_accuracy: 0.4956 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "506/506 [==============================] - 27s 53ms/step - loss: 0.6745 - accuracy: 0.5890 - val_loss: 0.5196 - val_accuracy: 0.7640 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "506/506 [==============================] - 27s 53ms/step - loss: 0.5790 - accuracy: 0.7052 - val_loss: 0.3189 - val_accuracy: 0.8769 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "506/506 [==============================] - 27s 53ms/step - loss: 0.2997 - accuracy: 0.8863 - val_loss: 0.2693 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "506/506 [==============================] - 27s 54ms/step - loss: 0.2685 - accuracy: 0.9023 - val_loss: 0.2600 - val_accuracy: 0.9024 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "506/506 [==============================] - 27s 53ms/step - loss: 0.2473 - accuracy: 0.9138 - val_loss: 0.3051 - val_accuracy: 0.8949 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "506/506 [==============================] - 27s 53ms/step - loss: 0.2280 - accuracy: 0.9227 - val_loss: 0.3236 - val_accuracy: 0.8941 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "506/506 [==============================] - 27s 53ms/step - loss: 0.2142 - accuracy: 0.9294 - val_loss: 0.2538 - val_accuracy: 0.9083 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "506/506 [==============================] - 27s 54ms/step - loss: 0.2004 - accuracy: 0.9359 - val_loss: 0.2598 - val_accuracy: 0.9074 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "506/506 [==============================] - 27s 53ms/step - loss: 0.1992 - accuracy: 0.9364 - val_loss: 0.2858 - val_accuracy: 0.9016 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "506/506 [==============================] - 27s 53ms/step - loss: 0.1464 - accuracy: 0.9616 - val_loss: 0.3009 - val_accuracy: 0.9097 - lr: 1.0000e-04\n",
      "Epoch 16/20\n",
      "506/506 [==============================] - 27s 53ms/step - loss: 0.1331 - accuracy: 0.9668 - val_loss: 0.3198 - val_accuracy: 0.9088 - lr: 1.0000e-04\n",
      "Epoch 17/20\n",
      "506/506 [==============================] - 27s 53ms/step - loss: 0.1251 - accuracy: 0.9711 - val_loss: 0.3349 - val_accuracy: 0.9102 - lr: 1.0000e-04\n",
      "Epoch 18/20\n",
      "506/506 [==============================] - 27s 54ms/step - loss: 0.1189 - accuracy: 0.9722 - val_loss: 0.3294 - val_accuracy: 0.9091 - lr: 1.0000e-04\n",
      "Epoch 19/20\n",
      "506/506 [==============================] - 27s 53ms/step - loss: 0.1139 - accuracy: 0.9750 - val_loss: 0.3756 - val_accuracy: 0.9058 - lr: 1.0000e-04\n",
      "Epoch 20/20\n",
      "506/506 [==============================] - 27s 53ms/step - loss: 0.1095 - accuracy: 0.9769 - val_loss: 0.3757 - val_accuracy: 0.9058 - lr: 1.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 11:24:13.988303: W tensorflow/core/util/tensor_slice_reader.cc:97] Could not open ./model/bestmodel: FAILED_PRECONDITION: model/bestmodel; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 2s 12ms/step\n",
      "PLeccNET\n",
      "TP: 1857 FP: 142 TN: 1795 FN: 204\n",
      "ACC: 0.9134567283641821 F1: 0.9147783251231527 MCC: 0.8273114734551081\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, MaxPool1D, Flatten, Dropout, Conv1D,Reshape,Input,AveragePooling1D,LSTM,Attention,MultiHeadAttention,MaxPooling1D,GlobalMaxPooling1D,Concatenate,BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers,Model,optimizers\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.losses import binary_crossentropy, hinge\n",
    "callback4 = [callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10,\n",
    "                                             verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0),\n",
    "                 callbacks.ModelCheckpoint(filepath=r'./model/bestmodel', monitor='val_accuracy', verbose=0,mode = 'max',\n",
    "                                           save_best_only=True, save_weights_only=False)]\n",
    "tf.random.set_seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "lu = 450\n",
    "f = open('./ecc/arabidopsis/'+str(lu)+'/arabidopsis.csv')\n",
    "train_data = []\n",
    "label = []\n",
    "while(True):\n",
    "    tt = f.readline()[:-1]\n",
    "    if(len(tt) <= 1):\n",
    "        break\n",
    "    tt = tt.split(',')\n",
    "    dd = []\n",
    "    for j in tt[0].upper():\n",
    "        if(j == 'A'):\n",
    "            dd.append([0,0,0,1])\n",
    "        elif (j == 'G'):\n",
    "            dd.append([0,0,1,0])\n",
    "        elif (j == 'C'):\n",
    "            dd.append([0,1,0,0])\n",
    "        elif (j == 'T'):\n",
    "            dd.append([1,0,0,0])\n",
    "        else:\n",
    "            dd.append([0,0,0,0])\n",
    "    while(len(dd) < lu*2):\n",
    "        dd.append([0,0,0,0])\n",
    "    train_data.append(dd)\n",
    "\n",
    "    label.append(int(tt[1]))\n",
    "\n",
    "\n",
    "\n",
    "train_data = np.array(train_data)\n",
    "label = np.array(label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data,test_data,label,test_label=train_test_split(train_data,label,test_size=0.1,random_state=3753,stratify=label)\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_label = np.array(test_label)\n",
    "train_data = np.array(train_data)\n",
    "label = np.array(label)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inputs1 = Input(shape=(lu*2,4),name='1st_input')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_mod = Conv1D(128,4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "\n",
    "x_mod = Conv1D(64,4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(x_mod)\n",
    "\n",
    "x_mod = MaxPooling1D(2)(x_mod)\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "# x_mod = BatchNormalization()(x_mod)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "co1 = Conv1D(x_mod.shape[1],4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "co2 = Conv1D(x_mod.shape[1],16, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co3 = Conv1D(247,6, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co4 = Conv1D(247,8, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co5 = Conv1D(247,10, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "pool1 = GlobalMaxPooling1D()(co1)\n",
    "pool2 = GlobalMaxPooling1D()(co2)\n",
    "# pool3 = GlobalMaxPooling1D()(co3)\n",
    "# pool4 = GlobalMaxPooling1D()(co4)\n",
    "# pool5 = GlobalMaxPooling1D()(co5)\n",
    "\n",
    "# 使用FPN结构合并多尺度特征\n",
    "fn = tf.add(pool1, pool2)\n",
    "fn = Reshape((x_mod.shape[1], -1))(fn)\n",
    "\n",
    "\n",
    "x_mod = Concatenate()([fn, x_mod])\n",
    "x_mod = LSTM(64,return_sequences=True)(x_mod)\n",
    "\n",
    "\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "\n",
    "\n",
    "x_mod = Flatten()(x_mod)\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "x_mod = Dense(64, activation='sigmoid')(x_mod)\n",
    "out = Dense(1, activation='sigmoid')(x_mod)\n",
    "\n",
    "model = Model(inputs=[inputs1], outputs=[out])#合成模型\n",
    "\n",
    "\n",
    "# 自定义损失函数\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # 交叉熵损失\n",
    "    cross_entropy_loss = binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # Hinge Cross-Entropy Loss\n",
    "    hinge_loss = tf.losses.hinge(y_true, y_pred)\n",
    "    \n",
    "    # 将两个损失相加\n",
    "    total_loss = 0.95*cross_entropy_loss + 0.05*hinge_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss=custom_loss,\n",
    "              optimizer=optimizers.Adam(learning_rate=0.001,beta_1=0.9, beta_2=0.888, epsilon=1e-08),metrics=['accuracy'],\n",
    "              )\n",
    "\n",
    "\n",
    "\n",
    "train_history = model.fit(train_data,\n",
    "                          label,\n",
    "                          validation_split=0.1,\n",
    "                          epochs=20,\n",
    "                          batch_size=64,\n",
    "                          verbose=1,\n",
    "                          shuffle=True,\n",
    "                          callbacks = callback4)\n",
    "\n",
    "\n",
    "model.load_weights('./model/bestmodel')\n",
    "pl = model.predict(test_data)\n",
    "p_test_label = []\n",
    "\n",
    "count = 0\n",
    "for i in range(len(pl)):\n",
    "    if(pl[i] >= 0.5):\n",
    "        p_test_label.append(1)\n",
    "    else:\n",
    "        p_test_label.append(0)\n",
    "# print(p_test_label)\n",
    "TP, FP, TN, FN, ACC, F1, MCC = comparison(p_test_label, test_label)\n",
    "print('PLeccNET')\n",
    "print('TP:', TP, 'FP:', FP, 'TN:', TN, 'FN:', FN)\n",
    "print('ACC:', ACC, 'F1:', F1, 'MCC:', MCC) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b66de938-f0d0-467e-8e74-de972351c51a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.0.count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.count\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).keras_api.metrics.1.count\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35980, 1000, 4)\n",
      "(3998, 1000, 4)\n",
      "Epoch 1/20\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.4386 - accuracy: 0.8178INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 41s 71ms/step - loss: 0.4386 - accuracy: 0.8178 - val_loss: 0.3336 - val_accuracy: 0.8855 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "506/506 [==============================] - 30s 60ms/step - loss: 0.3188 - accuracy: 0.8921 - val_loss: 0.7618 - val_accuracy: 0.4958 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "506/506 [==============================] - 30s 60ms/step - loss: 0.6254 - accuracy: 0.6098 - val_loss: 0.3440 - val_accuracy: 0.8613 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "506/506 [==============================] - 30s 60ms/step - loss: 0.3246 - accuracy: 0.8708 - val_loss: 0.3002 - val_accuracy: 0.8855 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.2741 - accuracy: 0.8987INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 35s 69ms/step - loss: 0.2741 - accuracy: 0.8987 - val_loss: 0.2955 - val_accuracy: 0.8866 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "506/506 [==============================] - 30s 60ms/step - loss: 0.2515 - accuracy: 0.9086 - val_loss: 0.3183 - val_accuracy: 0.8702 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.2453 - accuracy: 0.9123INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 35s 69ms/step - loss: 0.2453 - accuracy: 0.9123 - val_loss: 0.2916 - val_accuracy: 0.8885 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.2254 - accuracy: 0.9226INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 35s 69ms/step - loss: 0.2254 - accuracy: 0.9226 - val_loss: 0.3207 - val_accuracy: 0.8916 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "506/506 [==============================] - 30s 60ms/step - loss: 0.2280 - accuracy: 0.9219 - val_loss: 0.3670 - val_accuracy: 0.8610 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "506/506 [==============================] - 30s 60ms/step - loss: 0.2210 - accuracy: 0.9255 - val_loss: 0.3084 - val_accuracy: 0.8911 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.2185 - accuracy: 0.9266INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 35s 69ms/step - loss: 0.2185 - accuracy: 0.9266 - val_loss: 0.2768 - val_accuracy: 0.9038 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.2054 - accuracy: 0.9339INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 35s 69ms/step - loss: 0.2054 - accuracy: 0.9339 - val_loss: 0.2746 - val_accuracy: 0.9041 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "506/506 [==============================] - 30s 60ms/step - loss: 0.2010 - accuracy: 0.9370 - val_loss: 0.2730 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "506/506 [==============================] - ETA: 0s - loss: 0.2058 - accuracy: 0.9338INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/bestmodel/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "506/506 [==============================] - 35s 69ms/step - loss: 0.2058 - accuracy: 0.9338 - val_loss: 0.2705 - val_accuracy: 0.9091 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "506/506 [==============================] - 31s 60ms/step - loss: 0.1919 - accuracy: 0.9406 - val_loss: 0.4023 - val_accuracy: 0.8646 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "506/506 [==============================] - 30s 60ms/step - loss: 0.1959 - accuracy: 0.9389 - val_loss: 0.3353 - val_accuracy: 0.8808 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "506/506 [==============================] - 30s 60ms/step - loss: 0.1900 - accuracy: 0.9411 - val_loss: 0.2836 - val_accuracy: 0.9005 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "506/506 [==============================] - 30s 60ms/step - loss: 0.1878 - accuracy: 0.9442 - val_loss: 0.3235 - val_accuracy: 0.8986 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "506/506 [==============================] - 30s 60ms/step - loss: 0.1860 - accuracy: 0.9453 - val_loss: 0.3067 - val_accuracy: 0.9033 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "506/506 [==============================] - 30s 60ms/step - loss: 0.1827 - accuracy: 0.9444 - val_loss: 0.2876 - val_accuracy: 0.8966 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 11:37:52.048221: W tensorflow/core/util/tensor_slice_reader.cc:97] Could not open ./model/bestmodel: FAILED_PRECONDITION: model/bestmodel; Is a directory: perhaps your file is in a different file format and you need to use a different restore operator?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 2s 14ms/step\n",
      "PLeccNET\n",
      "TP: 1825 FP: 174 TN: 1797 FN: 202\n",
      "ACC: 0.9059529764882441 F1: 0.9066070541480378 MCC: 0.8119856111080338\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, MaxPool1D, Flatten, Dropout, Conv1D,Reshape,Input,AveragePooling1D,LSTM,Attention,MultiHeadAttention,MaxPooling1D,GlobalMaxPooling1D,Concatenate,BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras import regularizers,Model,optimizers\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras.losses import binary_crossentropy, hinge\n",
    "callback4 = [callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10,\n",
    "                                             verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0),\n",
    "                 callbacks.ModelCheckpoint(filepath=r'./model/bestmodel', monitor='val_accuracy', verbose=0,mode = 'max',\n",
    "                                           save_best_only=True, save_weights_only=False)]\n",
    "tf.random.set_seed(123)\n",
    "np.random.seed(123)\n",
    "\n",
    "lu = 500\n",
    "f = open('./ecc/arabidopsis/'+str(lu)+'/arabidopsis.csv')\n",
    "train_data = []\n",
    "label = []\n",
    "while(True):\n",
    "    tt = f.readline()[:-1]\n",
    "    if(len(tt) <= 1):\n",
    "        break\n",
    "    tt = tt.split(',')\n",
    "    dd = []\n",
    "    for j in tt[0].upper():\n",
    "        if(j == 'A'):\n",
    "            dd.append([0,0,0,1])\n",
    "        elif (j == 'G'):\n",
    "            dd.append([0,0,1,0])\n",
    "        elif (j == 'C'):\n",
    "            dd.append([0,1,0,0])\n",
    "        elif (j == 'T'):\n",
    "            dd.append([1,0,0,0])\n",
    "        else:\n",
    "            dd.append([0,0,0,0])\n",
    "    while(len(dd) < lu*2):\n",
    "        dd.append([0,0,0,0])\n",
    "    train_data.append(dd)\n",
    "\n",
    "    label.append(int(tt[1]))\n",
    "\n",
    "\n",
    "\n",
    "train_data = np.array(train_data)\n",
    "label = np.array(label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_data,test_data,label,test_label=train_test_split(train_data,label,test_size=0.1,random_state=3753,stratify=label)\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_label = np.array(test_label)\n",
    "train_data = np.array(train_data)\n",
    "label = np.array(label)\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "inputs1 = Input(shape=(lu*2,4),name='1st_input')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_mod = Conv1D(128,4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "\n",
    "x_mod = Conv1D(64,4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(x_mod)\n",
    "\n",
    "x_mod = MaxPooling1D(2)(x_mod)\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "# x_mod = BatchNormalization()(x_mod)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "co1 = Conv1D(x_mod.shape[1],4, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "co2 = Conv1D(x_mod.shape[1],16, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co3 = Conv1D(247,6, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co4 = Conv1D(247,8, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "# co5 = Conv1D(247,10, activation='relu',kernel_regularizer=regularizers.l2(0.001))(inputs1)\n",
    "pool1 = GlobalMaxPooling1D()(co1)\n",
    "pool2 = GlobalMaxPooling1D()(co2)\n",
    "# pool3 = GlobalMaxPooling1D()(co3)\n",
    "# pool4 = GlobalMaxPooling1D()(co4)\n",
    "# pool5 = GlobalMaxPooling1D()(co5)\n",
    "\n",
    "# 使用FPN结构合并多尺度特征\n",
    "fn = tf.add(pool1, pool2)\n",
    "fn = Reshape((x_mod.shape[1], -1))(fn)\n",
    "\n",
    "\n",
    "x_mod = Concatenate()([fn, x_mod])\n",
    "x_mod = LSTM(64,return_sequences=True)(x_mod)\n",
    "\n",
    "\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "\n",
    "\n",
    "x_mod = Flatten()(x_mod)\n",
    "x_mod = Dropout(0.1)(x_mod)\n",
    "x_mod = Dense(64, activation='sigmoid')(x_mod)\n",
    "out = Dense(1, activation='sigmoid')(x_mod)\n",
    "\n",
    "model = Model(inputs=[inputs1], outputs=[out])#合成模型\n",
    "\n",
    "\n",
    "# 自定义损失函数\n",
    "def custom_loss(y_true, y_pred):\n",
    "    # 交叉熵损失\n",
    "    cross_entropy_loss = binary_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    # Hinge Cross-Entropy Loss\n",
    "    hinge_loss = tf.losses.hinge(y_true, y_pred)\n",
    "    \n",
    "    # 将两个损失相加\n",
    "    total_loss = 0.95*cross_entropy_loss + 0.05*hinge_loss\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss=custom_loss,\n",
    "              optimizer=optimizers.Adam(learning_rate=0.001,beta_1=0.9, beta_2=0.888, epsilon=1e-08),metrics=['accuracy'],\n",
    "              )\n",
    "\n",
    "\n",
    "\n",
    "train_history = model.fit(train_data,\n",
    "                          label,\n",
    "                          validation_split=0.1,\n",
    "                          epochs=20,\n",
    "                          batch_size=64,\n",
    "                          verbose=1,\n",
    "                          shuffle=True,\n",
    "                          callbacks = callback4)\n",
    "\n",
    "\n",
    "model.load_weights('./model/bestmodel')\n",
    "pl = model.predict(test_data)\n",
    "p_test_label = []\n",
    "\n",
    "count = 0\n",
    "for i in range(len(pl)):\n",
    "    if(pl[i] >= 0.5):\n",
    "        p_test_label.append(1)\n",
    "    else:\n",
    "        p_test_label.append(0)\n",
    "# print(p_test_label)\n",
    "TP, FP, TN, FN, ACC, F1, MCC = comparison(p_test_label, test_label)\n",
    "print('PLeccNET')\n",
    "print('TP:', TP, 'FP:', FP, 'TN:', TN, 'FN:', FN)\n",
    "print('ACC:', ACC, 'F1:', F1, 'MCC:', MCC) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1629e927-4741-4470-a5ba-ec2674080a42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
